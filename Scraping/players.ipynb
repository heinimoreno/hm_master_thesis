{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of Players Information of Swiss Football Players\n",
    "\n",
    "## General Information\n",
    "**Objective**  \n",
    "The purpose of this Jupyter Notebook is to automate the process of extracting player information from Transfermarkt pages. The data collected will include personal details such as age, value, position, height, and more, enriching the previously scraped lineups data for deeper analysis.\n",
    "\n",
    "**Scope**  \n",
    "This script scrapes player data and saves raw CSV files in the `data/raw` subfolder. The collected data will later be cleaned, processed, and merged for advanced analysis.\n",
    "\n",
    "**Methodology**  \n",
    "The scraping process involves the following steps:\n",
    "- Defining URLs for data extraction.\n",
    "- Navigating Transfermarkt pages using web scraping techniques.\n",
    "- Collecting and structuring the scraped data.\n",
    "- Saving the structured data in CSV format for downstream analysis.\n",
    "\n",
    "**Usage**  \n",
    "The data extracted will contribute to a centralized dataset to facilitate insights into player statistics, transfers, and development trends. This structured dataset will be used in further stages of the project.\n",
    "\n",
    "---\n",
    "\n",
    "## Setting Up the Chrome WebDriver\n",
    "To scrape data from Transfermarkt pages, a compatible Chrome WebDriver must be installed.  \n",
    "- **Installation**: Download the WebDriver from [Google Chrome Labs](https://googlechromelabs.github.io/chrome-for-testing/).  \n",
    "- **Check Version**: Find your Chrome version in `Settings > About Google Chrome`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required WebDriver manager if not already installed\n",
    "#!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python Libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Data Manipulation Libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Web Scraping Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# OCR and Image Processing Libraries (for Market Value extraction)\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import pytesseract\n",
    "\n",
    "# Automation Libraries\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# Set the path to Tesseract OCR executable (adjust for your system)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Replace with your path\n",
    "\n",
    "# Set the path to your ChromeDriver. Adjust the path for your system.\n",
    "chrome_driver_directory = \"C:/Users/moreno/Downloads/chromedriver-win64\"  # Replace with your path\n",
    "os.environ[\"PATH\"] += os.pathsep + chrome_driver_directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Players Information\n",
    "\n",
    "\n",
    "**Objective**  \n",
    "Extract player information for various Swiss football teams from Transfermarkt, including personal data, position, and performance metrics.\n",
    "\n",
    "**Scope**  \n",
    "Scrape and save raw CSV files for players across multiple junior levels and seasons to create a comprehensive dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define junior levels and years for scraping\n",
    "junior_levels = ['28436', '28155', '23140', '22998', '19429', '9534', '3384']  # Junior team identifiers\n",
    "years = [2024, 2023, 2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009]  # Seasons\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Create an empty DataFrame to store player data\n",
    "players = pd.DataFrame()\n",
    "\n",
    "# Iterate over each junior level and season\n",
    "for junior_level in junior_levels:\n",
    "    for year in years:\n",
    "        # Construct the URL for the current junior level and year\n",
    "        url = f'https://www.transfermarkt.ch/schweiz-u15/kader/verein/{junior_level}/plus/1/galerie/0?saison_id={year}'\n",
    "        driver.get(url)  # Navigate to the webpage\n",
    "        time.sleep(2)    # Allow the page to load\n",
    "\n",
    "        # Handle cookie consent iframe if present\n",
    "        try:\n",
    "            wait = WebDriverWait(driver, 2.5)\n",
    "            iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953386\")))\n",
    "            driver.switch_to.frame(iframe)\n",
    "            accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "            accept_button.click()\n",
    "            driver.switch_to.default_content()\n",
    "        except Exception as e:\n",
    "            print(f\"No iframe found or error handling iframe for {year}: {e}\")\n",
    "\n",
    "        # Extract data from the table\n",
    "        try:\n",
    "            table = driver.find_element(By.XPATH, '//*[@id=\"yw1\"]/table')  # Find the main data table\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")  # Retrieve all rows\n",
    "            table_data = [[td.text for td in row.find_elements(By.TAG_NAME, \"td\")] for row in rows if len(row.find_elements(By.TAG_NAME, \"td\")) > 1]\n",
    "            \n",
    "            # If no data is found, raise an error\n",
    "            if not table_data:\n",
    "                raise ValueError(\"No data found for this year.\")\n",
    "            \n",
    "            # Convert table data to DataFrame and clean it\n",
    "            df = pd.DataFrame(table_data)\n",
    "            df.drop(df.columns[[0, 1, 2, 6, 10]], axis=1, inplace=True)  # Drop unnecessary columns\n",
    "            df.columns = ['Name', 'Position', 'Birthdate', 'Height', 'Foot', 'GamesPlayed', 'Debut', 'Value']  # Assign column names\n",
    "            df.dropna(how='all', inplace=True)  # Remove empty rows\n",
    "            df['Year'] = year\n",
    "            df['Category'] = junior_level\n",
    "            players = pd.concat([players, df], ignore_index=True)  # Append to main DataFrame\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process data for {year} at {junior_level}: {e}\")\n",
    "\n",
    "# Save the collected data to a CSV file\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "filename = f'../data/raw/players_NT_{current_date}.csv'\n",
    "players.to_csv(filename, index=False)\n",
    "\n",
    "# Close the WebDriver after scraping\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message and a preview of the data\n",
    "print(\"Web scraping successfully completed\")\n",
    "print(players.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Player IDs from Transfermarkt  \n",
    "\n",
    "**Objective**  \n",
    "This section retrieves unique player IDs from Transfermarkt. These IDs are essential for accessing individual player pages for further data extraction.\n",
    "\n",
    "**Scope**  \n",
    "Search each player by name and age to find the corresponding Transfermarkt profile and extract the unique Player ID.\n",
    "\n",
    "**Methodology** \n",
    "\n",
    "- Use players_unique.csv file which was cleaned in a first step - see data preparation script\n",
    "- Search each player's name on Transfermarkt.\n",
    "- Match names and ages to find the correct profile.\n",
    "- Extract and save the Player IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file containing unique player data\n",
    "df = pd.read_csv('../data/cleaned/players_unique.csv', encoding='utf-8')\n",
    "\n",
    "# Set up the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Function to handle cookie consent iframe\n",
    "def handle_iframe():\n",
    "    try:\n",
    "        # Wait for the iframe to load and switch to it\n",
    "        iframe = WebDriverWait(driver, 1.5).until(\n",
    "            EC.presence_of_element_located((By.ID, \"sp_message_iframe_953386\"))\n",
    "        )\n",
    "        driver.switch_to.frame(iframe)\n",
    "        \n",
    "        # Wait for the \"accept\" button in the iframe and click it\n",
    "        accept_button = WebDriverWait(driver, 1.3).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\"))\n",
    "        )\n",
    "        accept_button.click()\n",
    "        \n",
    "        # Switch back to the default content after handling the iframe\n",
    "        driver.switch_to.default_content()\n",
    "        print(\"Iframe handled successfully.\")\n",
    "    except Exception as e:\n",
    "        # If iframe is not found or an error occurs, print the exception\n",
    "        print(f\"No iframe found or error handling iframe: {e}\")\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "# Navigate to the Transfermarkt website and handle the initial cookie consent iframe\n",
    "driver.get(\"https://www.transfermarkt.ch/\")\n",
    "handle_iframe()\n",
    "\n",
    "# Iterate over each player in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Reset the state by navigating back to the main Transfermarkt page\n",
    "    driver.get(\"https://www.transfermarkt.ch/\")\n",
    "    handle_iframe()\n",
    "\n",
    "    # Locate the search bar, clear it, and enter the player's name\n",
    "    search_bar = WebDriverWait(driver, 1.3).until(\n",
    "        EC.visibility_of_element_located((By.XPATH, \"//*[@id='schnellsuche']/input\"))\n",
    "    )\n",
    "    search_bar.clear()  # Clear any existing text in the search bar\n",
    "    search_bar.send_keys(row['Name'])  # Enter the player's name\n",
    "    search_bar.submit()  # Submit the search query\n",
    "    \n",
    "    # Wait for the search results to load and process them\n",
    "    try:\n",
    "        # Locate all rows in the search results table\n",
    "        results = WebDriverWait(driver, 1.3).until(\n",
    "            EC.visibility_of_all_elements_located((By.XPATH, \"//*[@id='yw0']/table/tbody/tr\"))\n",
    "        )\n",
    "\n",
    "        player_found = False  # Flag to track if the player is found\n",
    "        \n",
    "        # First attempt: Match the name and age\n",
    "        for result in results:\n",
    "            try:\n",
    "                # Extract the name from the search result\n",
    "                name_in_result = result.find_element(By.XPATH, \"./td[1]/table/tbody/tr[1]/td[2]/a\").text\n",
    "                if name_in_result == row['Name']:\n",
    "                    # Extract and validate the player's age\n",
    "                    age_text = result.find_element(By.XPATH, \"./td[4]\").text\n",
    "                    age = int(age_text) if age_text.isdigit() else None\n",
    "\n",
    "                    if age == row['Age']:\n",
    "                        # Check for nationality (e.g., \"Schweiz\")\n",
    "                        nationality_elements = result.find_elements(By.XPATH, \"./td[5]/img\")\n",
    "                        nationalities = [img.get_attribute(\"title\") for img in nationality_elements]\n",
    "                        \n",
    "                        if \"Schweiz\" in nationalities:\n",
    "                            # Click the player's name and extract the Player ID from the URL\n",
    "                            result.find_element(By.XPATH, \"./td[1]/table/tbody/tr[1]/td[2]/a\").click()\n",
    "                            player_id = driver.current_url.split('/')[-1]\n",
    "                            df.at[index, 'Player ID'] = player_id\n",
    "                            player_found = True  # Update the flag\n",
    "                            break\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing details for {row['Name']} in one of the results: {e}\")\n",
    "\n",
    "        # Second attempt: Match using age only if no exact name match is found\n",
    "        if not player_found:\n",
    "            print(f\"No exact name match for {row['Name']}, attempting age-based match.\")\n",
    "            for result in results:\n",
    "                try:\n",
    "                    # Extract and validate the player's age\n",
    "                    age_text = result.find_element(By.XPATH, \"./td[4]\").text\n",
    "                    age = int(age_text) if age_text.isdigit() else None\n",
    "\n",
    "                    if age == row['Age']:\n",
    "                        # Click the player's name and extract the Player ID from the URL\n",
    "                        result.find_element(By.XPATH, \"./td[1]/table/tbody/tr[1]/td[2]/a\").click()\n",
    "                        player_id = driver.current_url.split('/')[-1]\n",
    "                        df.at[index, 'Player ID'] = player_id\n",
    "                        player_found = True  # Update the flag\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error accessing details for {row['Name']} in one of the results: {e}\")\n",
    "\n",
    "        # If the player is still not found, log it and mark as \"Not Found\"\n",
    "        if not player_found:\n",
    "            print(f\"Player {row['Name']} with age {row['Age']} not found.\")\n",
    "            df.at[index, 'Player ID'] = 'Not Found'\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle errors during the search process and mark the Player ID as \"Not Found\"\n",
    "        print(f\"Error processing {row['Name']}: {e}\")\n",
    "        df.at[index, 'Player ID'] = 'Not Found'\n",
    "\n",
    "# Save the updated DataFrame with the extracted Player IDs to a CSV file\n",
    "df.to_csv('../data/cleaned/players_incl_ID.csv', index=False)\n",
    "\n",
    "# Close the WebDriver after processing\n",
    "driver.quit()\n",
    "\n",
    "print(\"Player IDs added successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Player Stats Per National League and Season  \n",
    "\n",
    "**Objective**  \n",
    "Extract detailed player statistics, including games played, goals scored, assists, and other metrics, from Transfermarkt pages.\n",
    "\n",
    "**Scope**  \n",
    "Iterate through player profiles and extract data for each season and competition.\n",
    "\n",
    "**Methodology** \n",
    "\n",
    "- Use Player IDs to access individual pages.\n",
    "- Extract statistics for each season and competition type (e.g., national leagues, cups).\n",
    "- Save the data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two CSV files\n",
    "players_unique = pd.read_csv('../data/cleaned/players_cleaned.csv')\n",
    "players_incl_ID = pd.read_csv('../data/cleaned/players_incl_ID.csv')\n",
    "\n",
    "# Sort players_unique by date or another relevant column if available\n",
    "# Here, we assume sorting by index to get the last occurrence if there's no date column\n",
    "players_unique_sorted = players_unique.sort_index()\n",
    "\n",
    "# Drop duplicates in players_unique, keeping only the last occurrence of each 'Name'\n",
    "players_unique_last = players_unique_sorted.drop_duplicates(subset='Name', keep='last')\n",
    "\n",
    "# Merge the cleaned dataframe with players_incl_ID on 'Name'\n",
    "merged_df = players_incl_ID.merge(players_unique_last[['Name', 'Position']], on='Name', how='left')\n",
    "\n",
    "# Save the result to a new CSV file or display it\n",
    "merged_df.to_csv('../data/cleaned/players_incl_ID.csv', index=False)\n",
    "\n",
    "# Display the merged dataframe to verify\n",
    "print(merged_df.head())\n",
    "\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the CSV file containing Player IDs and their respective Positions\n",
    "df_players = pd.read_csv('../data/cleaned/players_incl_ID.csv')\n",
    "player_ids = df_players['Player ID'].unique()  # Extract unique Player IDs\n",
    "\n",
    "# Prepare an empty DataFrame to collect all player data\n",
    "all_players_data = pd.DataFrame()\n",
    "\n",
    "# Function to handle the correction of season formatting\n",
    "def correct_season_post_processing(season):\n",
    "    try:\n",
    "        # Handle cases where the season format includes text (e.g., '01-Feb')\n",
    "        if '-' in season and any(char.isalpha() for char in season):\n",
    "            parsed_date = datetime.strptime(season, \"%d-%b\")\n",
    "            return f\"{parsed_date.year - 1}/{parsed_date.year % 100:02d}\"  # Convert to a standard format\n",
    "        # Handle cases like '00/01' and correct to '00-01'\n",
    "        if '/' in season and len(season.split('/')) == 2:\n",
    "            return season.replace('/', '-')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return season  # Return the original if no changes are required\n",
    "\n",
    "# Define XPaths and types for tables containing the relevant statistics\n",
    "table_info = [\n",
    "    (\"//*[@id='yw1']/table/tbody\", \"Nationale Ligen\"),  # National leagues table\n",
    "    (\"//*[@id='yw2']/table/tbody\", \"Nationale Pokalwettbewerbe\"),  # National cups table\n",
    "    (\"//*[@id='yw3']/table/tbody\", \"Internationale Pokalwettbewerbe\")  # International competitions table\n",
    "]\n",
    "\n",
    "# Iterate over each player in the DataFrame\n",
    "for index, row in df_players.iterrows():\n",
    "    player_id = row['Player ID']  # Extract the Player ID\n",
    "    position = row['Position']  # Get the player's position (e.g., Goalkeeper, Outfield player)\n",
    "    \n",
    "    # Construct the URL to the player's detailed statistics page\n",
    "    player_url = f\"https://www.transfermarkt.ch/yanick-brecher/detaillierteleistungsdaten/spieler/{player_id}/plus/1\"\n",
    "    driver.get(player_url)  # Open the player's page\n",
    "    time.sleep(3)  # Allow the page to load fully\n",
    "    \n",
    "    # Handle cookie consent if required\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 2.5)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953386\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'message-component message-button no-children')]\")))\n",
    "        accept_button.click()\n",
    "        driver.switch_to.default_content()\n",
    "    except Exception as e:\n",
    "        print(f\"Cookie consent not handled: {e}\")\n",
    "\n",
    "    # Iterate over each type of table (national league, cups, international)\n",
    "    for xpath, table_type in table_info:\n",
    "        try:\n",
    "            wait = WebDriverWait(driver, 2.5)\n",
    "            table = wait.until(EC.presence_of_element_located((By.XPATH, xpath)))  # Locate the table\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")  # Get all rows in the table\n",
    "            \n",
    "            player_data = []  # Temporary list to store data for this player and table\n",
    "            for row in rows:\n",
    "                cols = row.find_elements(By.TAG_NAME, \"td\")  # Get all columns in the row\n",
    "                if cols and len(cols) > 10:  # Ensure sufficient columns are present\n",
    "                    # Extract and process relevant data from the row\n",
    "                    season = cols[0].text.strip()\n",
    "                    season = correct_season_post_processing(season)  # Correct the season format\n",
    "                    competition = cols[2].find_element(By.TAG_NAME, \"a\").text.strip()  # Competition name\n",
    "                    club_img = cols[3].find_element(By.TAG_NAME, \"img\")  # Club information\n",
    "                    club = club_img.get_attribute(\"alt\")  # Extract club name\n",
    "                    \n",
    "                    # Extract statistics based on the player's position\n",
    "                    if position == \"Torwart\":  # Goalkeeper-specific stats\n",
    "                        games_played = cols[4].text.strip()\n",
    "                        insgesamt = cols[5].text.strip()\n",
    "                        own_goals = cols[6].text.strip()\n",
    "                        substituted_on = cols[7].text.strip()\n",
    "                        substituted_off = cols[8].text.strip()\n",
    "                        yellow_cards = cols[9].text.strip()\n",
    "                        yellow_red = cols[10].text.strip()\n",
    "                        red_cards = cols[11].text.strip()\n",
    "                        goals_conceded = cols[12].text.strip()\n",
    "                        clean_sheets = cols[13].text.strip()\n",
    "                        played_minutes = cols[14].text.strip().replace(\"'\", \"\")\n",
    "                        \n",
    "                        # Append the extracted data to the list\n",
    "                        player_data.append([\n",
    "                            player_id, season, competition, club, games_played, insgesamt, own_goals, substituted_on, \n",
    "                            substituted_off, yellow_cards, yellow_red, red_cards, goals_conceded, clean_sheets, played_minutes, table_type\n",
    "                        ])\n",
    "                    else:  # Outfield player-specific stats\n",
    "                        games_played = cols[4].text.strip()\n",
    "                        goals = cols[5].text.strip()\n",
    "                        assists = cols[6].text.strip()\n",
    "                        own_goals = cols[7].text.strip()\n",
    "                        substituted_on = cols[8].text.strip()\n",
    "                        substituted_off = cols[9].text.strip()\n",
    "                        yellow_cards = cols[10].text.strip()\n",
    "                        yellow_red = cols[11].text.strip()\n",
    "                        red_cards = cols[12].text.strip()\n",
    "                        penalty_goals = cols[13].text.strip()\n",
    "                        minutes_per_goal = cols[14].text.strip()\n",
    "                        played_minutes = cols[15].text.strip().replace(\"'\", \"\")\n",
    "                        \n",
    "                        # Append the extracted data to the list\n",
    "                        player_data.append([\n",
    "                            player_id, season, competition, club, games_played, goals, assists, own_goals, substituted_on, \n",
    "                            substituted_off, yellow_cards, yellow_red, red_cards, penalty_goals, minutes_per_goal, played_minutes, table_type\n",
    "                        ])\n",
    "            \n",
    "            # Append the player's data to the main DataFrame\n",
    "            if player_data:\n",
    "                if position == \"Torwart\":  # Define goalkeeper-specific columns\n",
    "                    columns = [\n",
    "                        'Player ID', 'Season', 'Competition', 'Club', 'Games Played', 'insgesamt', 'Own Goals', \n",
    "                        'Substituted On', 'Substituted Off', 'Yellow Cards', 'Yellow Red', 'Red Cards', 'Goals Conceded', \n",
    "                        'Clean Sheets', 'Played Minutes', 'Type'\n",
    "                    ]\n",
    "                else:  # Define outfield player-specific columns\n",
    "                    columns = [\n",
    "                        'Player ID', 'Season', 'Competition', 'Club', 'Games Played', 'Goals', 'Assists', 'Own Goals', \n",
    "                        'Substituted On', 'Substituted Off', 'Yellow Cards', 'Yellow Red', 'Red Cards', 'Penalty Goals', \n",
    "                        'Minutes per Goal', 'Played Minutes', 'Type'\n",
    "                    ]\n",
    "                df_temp = pd.DataFrame(player_data, columns=columns)\n",
    "                \n",
    "                # Ensure consistent structure by filling missing columns with 0\n",
    "                all_columns = [\n",
    "                    'Player ID', 'Season', 'Competition', 'Club', 'Games Played', 'insgesamt', 'Goals', 'Assists', 'Own Goals', \n",
    "                    'Substituted On', 'Substituted Off', 'Yellow Cards', 'Yellow Red', 'Red Cards', 'Penalty Goals', \n",
    "                    'Minutes per Goal', 'Goals Conceded', 'Clean Sheets', 'Played Minutes', 'Type'\n",
    "                ]\n",
    "                df_temp = df_temp.reindex(columns=all_columns, fill_value=0)\n",
    "                \n",
    "                all_players_data = pd.concat([all_players_data, df_temp], ignore_index=True)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract data for player ID {player_id} from table {table_type}: {e}\")\n",
    "\n",
    "# Close the WebDriver after the scraping is complete\n",
    "driver.quit()\n",
    "\n",
    "# Apply season correction to the collected data\n",
    "all_players_data['Season'] = all_players_data['Season'].apply(correct_season_post_processing)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "all_players_data.to_csv('../data/raw/players_club_stats.csv', index=False)\n",
    "\n",
    "# Print a success message and preview the data\n",
    "print(\"Web scraping successfully completed\")\n",
    "print(all_players_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping National Team Call-Ups and Goals  \n",
    "\n",
    "**Objective**  \n",
    "Collect data on national team call-ups and goals scored for each player.\n",
    "\n",
    "**Scope**  \n",
    "Use Player IDs to extract data on matches, call-ups, and statistics for different national team levels.\n",
    "\n",
    "**Methodology**  \n",
    "\n",
    "- Access national team statistics pages using Player IDs.\n",
    "- Extract data on call-ups, goals scored, and matches played.\n",
    "- Save the data for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the CSV file containing Player IDs\n",
    "df_players = pd.read_csv('../data/cleaned/players_incl_ID.csv')  # Read the player data\n",
    "player_ids = df_players['Player ID'].unique()  # Extract unique Player IDs\n",
    "\n",
    "# Prepare an empty DataFrame to collect all player data\n",
    "all_players_data = pd.DataFrame()\n",
    "\n",
    "# Iterate over each Player ID to scrape national team data\n",
    "for player_id in player_ids:\n",
    "    # Construct the URL for the player's national team page\n",
    "    player_url = f\"https://www.transfermarkt.ch/johan-vonlanthen/nationalmannschaft/spieler/{player_id}\"\n",
    "    \n",
    "    # Navigate to the player's national team page\n",
    "    driver.get(player_url)\n",
    "    time.sleep(3)  # Allow the page to load fully\n",
    "    \n",
    "    # Handle cookie consent if needed\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 1)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953386\")))  # Locate the cookie iframe\n",
    "        driver.switch_to.frame(iframe)  # Switch to the iframe\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'message-component message-button no-children')]\")))  # Locate and click the accept button\n",
    "        accept_button.click()\n",
    "        driver.switch_to.default_content()  # Switch back to the main content\n",
    "    except Exception as e:\n",
    "        print(f\"Cookie consent not handled: {e}\")  # Print an error message if cookie consent handling fails\n",
    "\n",
    "    # Initialize a list to store the player's national team data\n",
    "    player_data = []\n",
    "    try:\n",
    "        # Attempt to locate the primary table containing national team data\n",
    "        wait = WebDriverWait(driver, 2)\n",
    "        table = wait.until(EC.presence_of_element_located((By.XPATH, \"//*[@id='tm-main']/div[2]/div[1]/div[1]/table\")))\n",
    "    except Exception:\n",
    "        try:\n",
    "            # If the first XPath fails, try an alternative XPath for the table\n",
    "            table = wait.until(EC.presence_of_element_located((By.XPATH, \"//*[@id='tm-main']/div[1]/div[1]/div[1]/table\")))\n",
    "        except Exception:\n",
    "            # If both attempts fail, log the issue and add a \"not found\" entry\n",
    "            print(f\"No data found for player ID {player_id}. Adding 'not found' entry.\")\n",
    "            player_data.append([player_id, \"not found\", \"not found\", \"not found\", \"not found\"])\n",
    "\n",
    "    # Only parse the table if it was successfully located\n",
    "    if not player_data:  # Ensure no \"not found\" entries before processing the table\n",
    "        try:\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")  # Locate all rows in the table\n",
    "            for row in rows:\n",
    "                cols = row.find_elements(By.TAG_NAME, \"td\")  # Get all columns in the row\n",
    "                if cols and len(cols) >= 6:  # Ensure the row has enough columns\n",
    "                    # Extract relevant data from the columns\n",
    "                    national_team = cols[2].text  # Name of the national team\n",
    "                    debut_date = cols[3].text  # Date of the player's debut\n",
    "                    games_played = cols[4].text  # Total games played\n",
    "                    goals_scored = cols[5].text  # Total goals scored\n",
    "                    \n",
    "                    # Append the extracted data to the player_data list\n",
    "                    player_data.append([player_id, national_team, debut_date, games_played, goals_scored])\n",
    "        except Exception as e:\n",
    "            # Log any errors encountered during table parsing\n",
    "            print(f\"Failed to parse data for player ID {player_id}: {e}\")\n",
    "            player_data.append([player_id, \"not found\", \"not found\", \"not found\", \"not found\"])\n",
    "\n",
    "    # Add the extracted data for the player to the main DataFrame\n",
    "    if player_data:\n",
    "        df_temp = pd.DataFrame(player_data, columns=['Player ID', 'National Team', 'Debut Date', 'Games Played', 'Goals Scored'])\n",
    "        all_players_data = pd.concat([all_players_data, df_temp], ignore_index=True)\n",
    "\n",
    "# Save the collected data to a CSV file\n",
    "all_players_data.to_csv('../data/raw/players_NT_stats.csv', index=False)\n",
    "\n",
    "# Close the WebDriver after scraping is complete\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message and show the first few rows of the collected data\n",
    "print(\"Webscraping successfully completed\")\n",
    "print(all_players_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting National Team IDs  \n",
    "**Objective** \n",
    "Retrieve unique IDs for national teams associated with players, enabling detailed analysis of national team statistics.\n",
    "\n",
    "**Scope**  \n",
    "Identify and extract National Team IDs from Transfermarkt using team names and search functionalities.\n",
    "\n",
    "**Methodology**\n",
    "\n",
    "- Use team names from the player data to search Transfermarkt.\n",
    "- Locate the national team's page and extract its unique ID.\n",
    "- Handle multiple attempts to find IDs for missing teams.\n",
    "- Save the IDs alongside the player data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file containing player national team data\n",
    "df = pd.read_csv('../data/raw/players_NT_stats.csv')\n",
    "\n",
    "# Extract unique national team names\n",
    "unique_teams = df['National Team'].unique()\n",
    "\n",
    "# Set up the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Function to handle cookie consent iframe\n",
    "def handle_iframe():\n",
    "    try:\n",
    "        # Wait for the iframe to load and switch to it\n",
    "        iframe = WebDriverWait(driver, 1.5).until(\n",
    "            EC.presence_of_element_located((By.ID, \"sp_message_iframe_953386\"))\n",
    "        )\n",
    "        driver.switch_to.frame(iframe)\n",
    "        # Wait for the \"accept\" button to become clickable and click it\n",
    "        accept_button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\"))\n",
    "        )\n",
    "        accept_button.click()\n",
    "        # Switch back to the main page content\n",
    "        driver.switch_to.default_content()\n",
    "        print(\"Iframe handled successfully.\")\n",
    "    except Exception as e:\n",
    "        # Log if iframe handling fails\n",
    "        print(f\"No iframe found or error handling iframe: {e}\")\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "# Navigate to Transfermarkt and handle the initial cookie consent\n",
    "driver.get(\"https://www.transfermarkt.ch/\")\n",
    "handle_iframe()\n",
    "\n",
    "# Dictionary to store mappings of national team names to their IDs\n",
    "team_id_dict = {}\n",
    "\n",
    "# First attempt to find National Team IDs\n",
    "for team in unique_teams:\n",
    "    # Navigate back to the Transfermarkt homepage\n",
    "    driver.get(\"https://www.transfermarkt.ch/\")\n",
    "    handle_iframe()\n",
    "\n",
    "    # Locate and interact with the search bar\n",
    "    search_bar = WebDriverWait(driver, 2).until(\n",
    "        EC.visibility_of_element_located((By.XPATH, \"//*[@id='schnellsuche']/input\"))\n",
    "    )\n",
    "    search_bar.clear()  # Clear any existing text in the search bar\n",
    "    search_bar.send_keys(team)  # Enter the team name\n",
    "    search_bar.submit()  # Submit the search query\n",
    "    \n",
    "    try:\n",
    "        # Wait for search results to load\n",
    "        rows = WebDriverWait(driver, 2).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//*[@id='yw0']/table/tbody/tr\"))\n",
    "        )\n",
    "        \n",
    "        found = False  # Flag to track if the team was found\n",
    "        for i, row in enumerate(rows, start=1):\n",
    "            # Construct XPath to find team names in the search results\n",
    "            team_name_xpath = f\"//*[@id='yw0']/table/tbody/tr[{i}]/td[2]/table/tbody/tr[1]/td/a\"\n",
    "            team_name_element = driver.find_element(By.XPATH, team_name_xpath)\n",
    "            # Match team name (case-insensitive) and navigate to its page\n",
    "            if team_name_element.text.strip().lower() == team.strip().lower():\n",
    "                team_name_element.click()\n",
    "                # Extract the National Team ID from the URL\n",
    "                team_id = driver.current_url.split('/')[-1]\n",
    "                team_id_dict[team] = team_id\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            # If the team is not found in the first search results, mark as \"Not Found\"\n",
    "            team_id_dict[team] = 'Not Found'\n",
    "            print(f\"{team} not found in first search results.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle errors during search processing\n",
    "        print(f\"Error processing {team}: {e}\")\n",
    "        team_id_dict[team] = 'Not Found'\n",
    "\n",
    "# Map the found IDs back to the DataFrame\n",
    "df['National Team ID'] = df['National Team'].map(team_id_dict)\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('../data/cleaned/players_NT_stats_incl_ID.csv', index=False)\n",
    "\n",
    "# Second attempt for teams not found in the first round\n",
    "not_found_teams = df[df['National Team ID'] == 'Not Found']['National Team'].unique()\n",
    "\n",
    "for team in not_found_teams:\n",
    "    driver.get(\"https://www.transfermarkt.ch/\")\n",
    "    handle_iframe()\n",
    "\n",
    "    search_bar = WebDriverWait(driver, 2).until(\n",
    "        EC.visibility_of_element_located((By.XPATH, \"//*[@id='schnellsuche']/input\"))\n",
    "    )\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(team)\n",
    "    search_bar.submit()\n",
    "    \n",
    "    try:\n",
    "        rows = WebDriverWait(driver, 2).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//*[@id='yw2']/table/tbody/tr\"))\n",
    "        )\n",
    "        \n",
    "        found = False\n",
    "        for i, row in enumerate(rows, start=1):\n",
    "            team_name_xpath = f\"//*[@id='yw2']/table/tbody/tr[{i}]/td[2]/table/tbody/tr[1]/td/a\"\n",
    "            team_name_element = driver.find_element(By.XPATH, team_name_xpath)\n",
    "            if team_name_element.text.strip().lower() == team.strip().lower():\n",
    "                team_name_element.click()\n",
    "                team_id = driver.current_url.split('/')[-1]\n",
    "                df.loc[df['National Team'] == team, 'National Team ID'] = team_id\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"{team} not found in second search results.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {team} in second attempt: {e}\")\n",
    "\n",
    "df.to_csv('../data/raw/players_NT_stats_incl_ID.csv', index=False)\n",
    "\n",
    "# Third attempt for teams still not found\n",
    "not_found_teams = df[df['National Team ID'] == 'Not Found']['National Team'].unique()\n",
    "\n",
    "for team in not_found_teams:\n",
    "    driver.get(\"https://www.transfermarkt.ch/\")\n",
    "    handle_iframe()\n",
    "\n",
    "    search_bar = WebDriverWait(driver, 2).until(\n",
    "        EC.visibility_of_element_located((By.XPATH, \"//*[@id='schnellsuche']/input\"))\n",
    "    )\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(team)\n",
    "    search_bar.submit()\n",
    "    \n",
    "    try:\n",
    "        rows = WebDriverWait(driver, 2).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//*[@id='yw1']/table/tbody/tr\"))\n",
    "        )\n",
    "        \n",
    "        found = False\n",
    "        for i, row in enumerate(rows, start=1):\n",
    "            team_name_xpath = f\"//*[@id='yw1']/table/tbody/tr[{i}]/td[2]/table/tbody/tr[1]/td/a\"\n",
    "            team_name_element = driver.find_element(By.XPATH, team_name_xpath)\n",
    "            if team_name_element.text.strip().lower() == team.strip().lower():\n",
    "                team_name_element.click()\n",
    "                team_id = driver.current_url.split('/')[-1]\n",
    "                df.loc[df['National Team'] == team, 'National Team ID'] = team_id\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"{team} not found in third search results.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {team} in third attempt: {e}\")\n",
    "\n",
    "df.to_csv('../data/raw/players_NT_stats_incl_ID.csv', index=False)\n",
    "\n",
    "# Close the WebDriver after completing the process\n",
    "driver.quit()\n",
    "\n",
    "print(\"National Team IDs updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping National Team Playing Minutes  \n",
    "\n",
    "**Objective**  \n",
    "Collect detailed playing minutes statistics for players in their national teams, categorized by competition and season.\n",
    "\n",
    "**Scope**  \n",
    "Extract statistics such as games played, goals scored, and minutes on the field for each player in their respective national teams.\n",
    "\n",
    "**Methodology**  \n",
    "\n",
    "- Use Player IDs and National Team IDs to access detailed pages.\n",
    "- Extract season-wise performance data for each competition.\n",
    "- Save the collected data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the files containing Player ID, Position, and National Team ID\n",
    "df_positions = pd.read_csv('../data/cleaned/players_incl_ID.csv')  # Contains Player ID and Position\n",
    "df_national_teams = pd.read_csv('../data/raw/players_NT_stats_incl_ID.csv')  # Contains Player ID and National Team ID\n",
    "\n",
    "# Merge player and national team data to associate each Player ID with multiple National Team IDs and Position\n",
    "df_players = pd.merge(df_national_teams, df_positions[['Player ID', 'Position']], on='Player ID', how='left')\n",
    "\n",
    "# Prepare an empty DataFrame to collect all player data\n",
    "all_players_data = pd.DataFrame()\n",
    "\n",
    "# Function to correct season formatting to a standard format\n",
    "def correct_season_post_processing(season):\n",
    "    try:\n",
    "        # Handle cases with text dates like '01-Feb'\n",
    "        if '-' in season and any(char.isalpha() for char in season):\n",
    "            parsed_date = datetime.strptime(season, \"%d-%b\")\n",
    "            return f\"{parsed_date.year - 1}/{parsed_date.year % 100:02d}\"\n",
    "        # Handle cases like '00/01' and convert to '00-01'\n",
    "        if '/' in season and len(season.split('/')) == 2:\n",
    "            return season.replace('/', '-')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return season  # Return the original if no formatting changes are required\n",
    "\n",
    "# Iterate over each Player ID and National Team ID in the merged DataFrame\n",
    "for index, row in df_players.iterrows():\n",
    "    player_id = row['Player ID']  # Extract Player ID\n",
    "    national_team_id = row['National Team ID']  # Extract National Team ID for each player\n",
    "    position = row['Position']  # Extract the player's position (e.g., Goalkeeper or Outfield)\n",
    "\n",
    "    # Construct the URL for the player's national team statistics page\n",
    "    player_url = f\"https://www.transfermarkt.ch/yanick-brecher/nationalmannschaft/spieler/{player_id}/plus/1/verein_id/{national_team_id}\"\n",
    "    driver.get(player_url)  # Navigate to the player's statistics page\n",
    "    time.sleep(3)  # Allow the page to fully load\n",
    "    \n",
    "    # Handle cookie consent if needed\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 2.5)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953386\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'message-component message-button no-children')]\")))\n",
    "        accept_button.click()\n",
    "        driver.switch_to.default_content()\n",
    "    except Exception as e:\n",
    "        print(f\"Cookie consent not handled or already accepted: {e}\")\n",
    "\n",
    "    # Try to locate the main table containing national team statistics\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 2.5)\n",
    "        table = wait.until(EC.presence_of_element_located((By.XPATH, \"//*[@id='yw0']/table\")))  # Locate the table\n",
    "        rows = table.find_elements(By.TAG_NAME, \"tr\")  # Extract all rows from the table\n",
    "        \n",
    "        player_data = []  # Initialize a list to store the player's data\n",
    "        for row in rows:\n",
    "            cols = row.find_elements(By.TAG_NAME, \"td\")  # Extract columns from the row\n",
    "            if cols and len(cols) > 10:  # Ensure there are enough columns to extract data\n",
    "                season = cols[0].text.strip()  # Extract the season\n",
    "                season = correct_season_post_processing(season)  # Format the season\n",
    "                competition = cols[1].text.strip()  # Extract the competition name\n",
    "                \n",
    "                # Extract data based on player position\n",
    "                if position == \"Torwart\":  # Goalkeeper-specific stats\n",
    "                    games_played = cols[2].text.strip()\n",
    "                    goals = cols[3].text.strip()                \n",
    "                    own_goals = cols[4].text.strip()             \n",
    "                    substituted_on = cols[5].text.strip()              \n",
    "                    substituted_off = cols[6].text.strip()             \n",
    "                    yellow_cards = cols[7].text.strip()                \n",
    "                    yellow_red = cols[8].text.strip()                  \n",
    "                    red_cards = cols[9].text.strip()                   \n",
    "                    goals_conceded = cols[10].text.strip()               \n",
    "                    clean_sheets = cols[11].text.strip()    \n",
    "                    played_minutes = cols[12].text.strip().replace(\"'\", \"\")  \n",
    "                    \n",
    "                    player_data.append([\n",
    "                        player_id, national_team_id, position, season, competition, games_played, goals, own_goals, substituted_on, substituted_off, \n",
    "                        yellow_cards, yellow_red, red_cards, goals_conceded, clean_sheets, played_minutes\n",
    "                    ])\n",
    "                else:  # Outfield player-specific stats\n",
    "                    games_played = cols[2].text.strip()\n",
    "                    goals = cols[3].text.strip()\n",
    "                    assists = cols[4].text.strip()\n",
    "                    own_goals = cols[5].text.strip()\n",
    "                    substituted_on = cols[6].text.strip()\n",
    "                    substituted_off = cols[7].text.strip()\n",
    "                    yellow_cards = cols[8].text.strip()\n",
    "                    yellow_red = cols[9].text.strip()\n",
    "                    red_cards = cols[10].text.strip()\n",
    "                    penalty_goals = cols[11].text.strip()\n",
    "                    minutes_per_goal = cols[12].text.strip()\n",
    "                    played_minutes = cols[13].text.strip().replace(\"'\", \"\")\n",
    "                    \n",
    "                    player_data.append([\n",
    "                        player_id, national_team_id, position, season, competition, games_played, goals, assists, own_goals, substituted_on, substituted_off, \n",
    "                        yellow_cards, yellow_red, red_cards, penalty_goals, minutes_per_goal, played_minutes\n",
    "                    ])\n",
    "        \n",
    "        # Append the extracted data for the player to the main DataFrame\n",
    "        if player_data:\n",
    "            if position == \"Torwart\":  # Define goalkeeper-specific columns\n",
    "                columns = [\n",
    "                    'Player ID', 'National Team ID', 'Position', 'Season', 'Competition', 'Games Played', 'Goals', 'Own Goals', 'Substituted On', 'Substituted Off', \n",
    "                    'Yellow Cards', 'Yellow Red', 'Red Cards', 'Goals Conceded', 'Clean Sheets', 'Played Minutes'\n",
    "                ]\n",
    "            else:  # Define outfield player-specific columns\n",
    "                columns = [\n",
    "                    'Player ID', 'National Team ID', 'Position', 'Season', 'Competition', 'Games Played', 'Goals', 'Assists', 'Own Goals', 'Substituted On', 'Substituted Off', \n",
    "                    'Yellow Cards', 'Yellow Red', 'Red Cards', 'Penalty Goals', 'Minutes per Goal', 'Played Minutes'\n",
    "                ]\n",
    "            df_temp = pd.DataFrame(player_data, columns=columns)  # Create a temporary DataFrame\n",
    "            \n",
    "            # Ensure all columns exist, fill missing values with \"-\"\n",
    "            all_columns = [\n",
    "                'Player ID', 'National Team ID', 'Position', 'Season', 'Competition', 'Games Played', 'Goals', 'Assists', 'Own Goals', 'Substituted On', \n",
    "                'Substituted Off', 'Yellow Cards', 'Yellow Red', 'Red Cards', 'Penalty Goals', 'Minutes per Goal', 'Goals Conceded', 'Clean Sheets', \n",
    "                 'Played Minutes'\n",
    "            ]\n",
    "            df_temp = df_temp.reindex(columns=all_columns, fill_value=\"-\")  # Ensure consistent structure\n",
    "            \n",
    "            all_players_data = pd.concat([all_players_data, df_temp], ignore_index=True)  # Append to main DataFrame\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract data for player ID {player_id} and National Team ID {national_team_id}: {e}\")\n",
    "\n",
    "# Close the WebDriver after the scraping is complete\n",
    "driver.quit()\n",
    "\n",
    "# Correct season formatting in the final DataFrame\n",
    "all_players_data['Season'] = all_players_data['Season'].apply(correct_season_post_processing)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "all_players_data.to_csv('../data/raw/players_detailed_stats_NT.csv', index=False)\n",
    "\n",
    "# Print a success message and preview the data\n",
    "print(\"Web scraping successfully completed\")\n",
    "print(all_players_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Market Values for Each Player ID  \n",
    "\n",
    "**Objective**  \n",
    "Retrieve historical market value data for players from Transfermarkt, using tooltips to extract detailed information.\n",
    "\n",
    "**Scope**  \n",
    "Extract market values, dates, and associated club information for each player over time.\n",
    "\n",
    "**Methodology**\n",
    "\n",
    "- Use Player IDs to access market value trend pages.\n",
    "- Trigger tooltips on the graph for historical data.\n",
    "- Use OCR to extract tooltip text and parse market value details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Tesseract OCR executable (modify based on your installation)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Initialize Chrome WebDriver with options to maximize the window\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Temporarily set a larger viewport height to capture tooltips outside the visible area\n",
    "driver.set_window_size(1920, 1600)\n",
    "\n",
    "# Load the CSV file containing Player IDs\n",
    "df_players = pd.read_csv('../data/cleaned/players_incl_ID.csv')  # Load player data\n",
    "player_ids = df_players['Player ID'].unique()  # Extract unique Player IDs\n",
    "\n",
    "# List to store market value data for efficiency\n",
    "all_market_values = []\n",
    "\n",
    "# Function to parse text extracted from OCR output\n",
    "def parse_tooltip_from_text(text):\n",
    "    # Define replacements for common OCR errors\n",
    "    replacements = {\n",
    "        \"Verzin\": \"Verein\",\n",
    "        \"Aiter\": \"Alter\",\n",
    "        \"Variin\": \"Verein\",\n",
    "        \"Tsc\": \"Tsd.\",\n",
    "        \"Vio\": \"Mio.\",\n",
    "        \"Kio\": \"Mio.\",\n",
    "        \"lM\": \"Mio.\",\n",
    "        \"Om%\": \"2.5 Mio\",\n",
    "        \"Werbung\": \"\",\n",
    "        \"Anzeige\": \"\",\n",
    "        \"anne\": \"\",\n",
    "        \"Warhiing\": \"\",\n",
    "        \"anAan\": \"\",\n",
    "        \"Aktueller Marktwert\": \"\"\n",
    "    }\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "\n",
    "    # Clean the text and extract relevant fields using regex\n",
    "    text = re.sub(r'[^\\w\\s.,:()-]', '', text)  # Remove unwanted characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
    "\n",
    "    # Extract relevant data fields\n",
    "    date_match = re.search(r\"(\\d{2}\\.\\d{2}\\.\\d{4})\", text)  # Find date\n",
    "    market_value_match = re.search(r\"Marktwert:\\s*([0-9,.]+\\s*(Mio\\.|Tsd\\.))\", text, re.IGNORECASE)  # Find market value\n",
    "    club_match = re.search(r\"Verein:\\s*([^\\n]+)\", text, re.IGNORECASE)  # Find club name\n",
    "    age_match = re.search(r\"Alter:\\s*(\\d+)\", text, re.IGNORECASE)  # Find age\n",
    "\n",
    "    # Extract matched values or set them to None if not found\n",
    "    date = date_match.group(1) if date_match else None\n",
    "    market_value = market_value_match.group(1).replace(',', '.') if market_value_match else None\n",
    "    club = club_match.group(1).strip() if club_match else None\n",
    "    age = age_match.group(1) if age_match else None\n",
    "\n",
    "    # Clean the club name to remove unnecessary text\n",
    "    if club:\n",
    "        club = re.split(r\"(\\sAnzeige|\\sWerbung)\", club)[0].strip()\n",
    "\n",
    "    return date, market_value, club, age\n",
    "\n",
    "# Iterate over each Player ID to scrape market value data\n",
    "for player_id in player_ids:\n",
    "    player_url = f'https://www.transfermarkt.ch/hakan-yakin/marktwertverlauf/spieler/{player_id}'\n",
    "    print(f\"\\nProcessing Player ID: {player_id}\")\n",
    "    driver.get(player_url)\n",
    "\n",
    "    # Handle cookie consent iframe if present\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.frame_to_be_available_and_switch_to_it((By.ID, \"sp_message_iframe_953386\"))\n",
    "        )\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\"))\n",
    "        ).click()\n",
    "        driver.switch_to.default_content()\n",
    "    except Exception as e:\n",
    "        print(\"No cookie iframe found or error in handling iframe.\")\n",
    "\n",
    "    # Retrieve the current market value (last entry shown on the page)\n",
    "    try:\n",
    "        last_market_value = driver.find_element(By.XPATH, '//*[@id=\"tm-main\"]/div[2]/div[1]/div/tm-market-value-development-graph-extended/div/h3').text\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        all_market_values.append({\n",
    "            'Player_ID': player_id,\n",
    "            'OCR_Text': \"N/A\",  # No OCR data for this entry\n",
    "            'Date': current_date,\n",
    "            'Age': \"N/A\",  # Age can be derived later from other sources\n",
    "            'Market_Value': last_market_value,\n",
    "            'Club': \"See Player Profile\"\n",
    "        })\n",
    "        print(f\"Current market value data for {player_id} saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving last market value for Player {player_id}: {e}\")\n",
    "\n",
    "    # Locate the graph elements representing tooltip points\n",
    "    path_elements = driver.find_elements(By.CSS_SELECTOR, \"#tm-main > div.row > div.large-8.columns > div > tm-market-value-development-graph-extended > div > div > svg > g.svelte-14kpmtb > path\")\n",
    "\n",
    "    if not path_elements:\n",
    "        print(f\"No path elements found for Player {player_id}.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"{len(path_elements)} path elements found for Player {player_id}.\")\n",
    "    actions = ActionChains(driver)\n",
    "\n",
    "    # Process each tooltip element to extract market value data\n",
    "    for index, element in enumerate(path_elements):\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", element)  # Center the element in view\n",
    "        driver.execute_script(\"arguments[0].dispatchEvent(new MouseEvent('mouseover', {bubbles: true}));\", element)  # Trigger tooltip\n",
    "        time.sleep(1)  # Allow the tooltip to appear\n",
    "\n",
    "        # Capture screenshot of the tooltip\n",
    "        screenshot_path = f\"tooltip_{player_id}_{index}.png\"\n",
    "        driver.save_screenshot(screenshot_path)\n",
    "\n",
    "        # Process the screenshot using OCR\n",
    "        image = Image.open(screenshot_path)\n",
    "        crop_coordinates = (0, 0, 150, 115)  # Crop the tooltip region\n",
    "        cropped_image = image.crop(crop_coordinates)\n",
    "        cropped_image = cropped_image.resize((cropped_image.width * 4, cropped_image.height * 4), Image.LANCZOS)\n",
    "        cropped_image = ImageEnhance.Contrast(cropped_image.convert('L')).enhance(2)\n",
    "        cropped_image = cropped_image.filter(ImageFilter.SHARPEN)\n",
    "\n",
    "        # Extract text using OCR\n",
    "        tooltip_text = pytesseract.image_to_string(cropped_image, lang='de+eng').strip()\n",
    "\n",
    "        # Parse the OCR text to extract fields\n",
    "        date, market_value, club, age = parse_tooltip_from_text(tooltip_text)\n",
    "\n",
    "        # Delete the screenshot after processing to save space\n",
    "        os.remove(screenshot_path)\n",
    "\n",
    "        # Append valid data to the results list\n",
    "        if date or market_value or club or age:\n",
    "            all_market_values.append({\n",
    "                'Player_ID': player_id,\n",
    "                'OCR_Text': tooltip_text,\n",
    "                'Date': date,\n",
    "                'Age': age,\n",
    "                'Market_Value': market_value,\n",
    "                'Club': club\n",
    "            })\n",
    "            print(f\"Data added for Tooltip {index + 1} of Player {player_id}.\")\n",
    "        else:\n",
    "            print(f\"No valid data from Tooltip {index + 1} for Player {player_id}.\")\n",
    "\n",
    "# Save all collected data to a CSV file\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "filename = f'../data/raw/market_values_{current_date}.csv'\n",
    "df_final = pd.DataFrame(all_market_values)\n",
    "df_final.to_csv(filename, index=False)\n",
    "print(f\"\\nData saved in {filename}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "print(\"Web scraping completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
